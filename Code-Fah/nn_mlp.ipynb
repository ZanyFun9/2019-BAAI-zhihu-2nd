{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import softmax\n",
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "import gc\n",
    "import os\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold,RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_2_0926.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "target=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_topic_w2vdis=pd.read_hdf('feature_topic_w2vdis.h5', key='data')\n",
    "feature_member_info=pd.read_hdf('feature_member_info_new.h5', key='data')[['salt','sex_count','look_freq_count','A_count','B_count','D_count']]\n",
    "feature_invite_time_new=pd.read_hdf('feature_invite_time_new2.h5', key='data')\n",
    "feature_uq_pagerank=pd.read_hdf('feature_uq_pagerank2.h5', key='data')\n",
    "feature_today_invite=pd.read_hdf('feature_today_invite_new2.h5', key='data')\n",
    "feature_uid_ctr_new=pd.read_hdf('feature_uid_ctr_new.h5', key='data')\n",
    "feature_topic_count=pd.read_csv('feature_topic_count.csv')\n",
    "\n",
    "train=pd.concat([train,\n",
    "                 feature_topic_w2vdis.iloc[0:train.shape[0],:],\n",
    "                 feature_member_info.iloc[0:train.shape[0],:],\n",
    "                 feature_invite_time_new.iloc[0:train.shape[0],3:],\n",
    "                 feature_uq_pagerank.iloc[0:train.shape[0],:],\n",
    "                 feature_today_invite.iloc[0:train.shape[0],:],\n",
    "                 feature_topic_count.iloc[0:train.shape[0],:],\n",
    "                ],axis=1)\n",
    "test=pd.concat([test,\n",
    "                feature_topic_w2vdis.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_member_info.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_invite_time_new.iloc[train.shape[0]:,3:].reset_index(drop=True),\n",
    "                feature_uq_pagerank.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_today_invite.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_topic_count.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_qid_freq=pd.read_hdf('feature_qid_freq2.h5', key='data').drop_duplicates(['qid','uid'])\n",
    "train=pd.merge(train,feature_qid_freq,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_qid_freq,how='left',on=['qid','uid'])\n",
    "feature_uid_freq=pd.read_hdf('feature_uid_freq2.h5', key='data').drop_duplicates(['qid','uid'])\n",
    "train=pd.merge(train,feature_uid_freq,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_uid_freq,how='left',on=['qid','uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_answer_base=pd.read_hdf('feature_answer_base.h5', key='data')\n",
    "train=pd.merge(train,feature_answer_base,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_answer_base,how='left',on=['qid','uid'])\n",
    "feature_Stime_history=pd.read_hdf('feature_Stime_history.h5', key='data').drop_duplicates(['qid','uid'])\n",
    "train=pd.merge(train,feature_Stime_history,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_Stime_history,how='left',on=['qid','uid'])\n",
    "feature_atime_new=pd.read_hdf('feature_atime_new.h5', key='data') \n",
    "train=pd.merge(train,feature_atime_new,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_atime_new,how='left',on=['qid','uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    fea= Input(shape=(train.shape[1],))\n",
    "    fea1 = Dense(1024, activation='relu')(fea)\n",
    "    dense = Dropout(0.3)(fea1)\n",
    "    dense = Dense(512, activation='relu')(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    dense = Dense(512, activation='relu')(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    pred = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[fea], outputs=pred)#\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "n_splits= 5\n",
    "folds = StratifiedKFold(n_splits=n_splits)\n",
    "sub1 = np.zeros((test.shape[0],1))\n",
    "oof_pref1 = np.zeros((train.shape[0], 1))\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(['uid','qid','time'], axis=1)\n",
    "train = train.drop(['target', 'uid','qid','time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape=train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489162, 122)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1141718, 122)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10630880"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "mm = StandardScaler()\n",
    "alldata = mm.fit_transform(pd.concat([train,test]).replace([np.inf, -np.inf], 0).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19652388, -0.14816786, -0.41910499, ..., -1.33222188,\n",
       "        -0.55826741, -1.20170751],\n",
       "       [ 0.98222788,  1.04079583,  0.15003249, ..., -1.33222188,\n",
       "        -0.55826741, -1.20170751],\n",
       "       [ 1.3073875 ,  1.41647053,  0.63756395, ...,  0.58949734,\n",
       "         1.40293978,  0.9942194 ],\n",
       "       ...,\n",
       "       [ 0.68809863,  0.7632955 ,  0.15104319, ..., -0.56353419,\n",
       "         0.74920405, -0.10374405],\n",
       "       [-1.84760627, -0.49152014,  2.0443155 , ..., -1.33222188,\n",
       "        -0.55826741, -1.20170751],\n",
       "       [ 0.43056048,  0.40789563, -0.16890455, ..., -1.33222188,\n",
       "        -0.55826741, -1.20170751]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"all_data_final.npy\", alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = alldata[0:train_shape[0],:]#.reset_index(drop=True)\n",
    "test = alldata[train_shape[0]:,:]#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1217 16:37:45.769602 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:92: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1217 16:37:45.770889 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1217 16:37:45.822329 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1217 16:37:45.823595 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1217 16:37:45.826223 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1217 16:37:45.848031 140122212017920 deprecation.py:506] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1217 16:37:45.943799 140122212017920 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD_  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1217 16:37:45.988299 140122212017920 deprecation.py:323] From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/15\n",
      "7591329/7591329 [==============================] - 68s 9us/step - loss: 0.3685 - acc: 0.8405 - val_loss: 0.3530 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35303, saving model to mlp0.h5\n",
      "Epoch 2/15\n",
      "4603904/7591329 [=================>............] - ETA: 24s - loss: 0.3537 - acc: 0.8466"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(oof_pref1, target)):\n",
    "    print(\"FOLD_ \", count + 1)\n",
    "    K.clear_session()\n",
    "    filepath = \"mlp%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=2, verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    model_nn =build_model()\n",
    "    y_tr,y_vl=target[trn_idx], target[val_idx]\n",
    "    train_fea_trn,train_fea_vl=train[trn_idx], train[val_idx]\n",
    "    gc.collect()\n",
    "    hist = model_nn.fit([train_fea_trn], y_tr, batch_size=1024, epochs=15, #\n",
    "                        validation_data=([train_fea_vl ], y_vl),callbacks=callbacks, verbose=1, shuffle=True)#,,\n",
    "    print('load_wight')\n",
    "    model_nn.load_weights(filepath)\n",
    "    print('start to predict')\n",
    "    sub1 += model_nn.predict([test], batch_size=2048)\n",
    "    oof_pref1[val_idx] = model_nn.predict([train_fea_vl], batch_size=2048)\n",
    "    count += 1\n",
    "    print('val_auc:',metrics.roc_auc_score(y_vl,oof_pref1[val_idx]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.843927  0.34398 0.813170 0.03'uday_sametime_icount','uday_invite_rank','qday_invite_rank','qday_sametime_icount',\n",
    "#0.8531  0.335  \n",
    "#0.8172  0.363     798     0.02  0.015(qd day i rank)\n",
    "#0.8409   0.3454  0.8085   0.0323 0.0046(u q sametime i)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.843927 0.813170  0.0307'uday_sametime_icount','uday_invite_rank','qday_invite_rank','qday_sametime_icount',\n",
    "#0.820    0.8018    0.0181   'uday_sametime_icount','uday_invite_rank','qday_invite_rank'\n",
    "#0.8172   0.798     0.02         (qd day i rank)\n",
    "#0.8409   0.8085   0.0323 0.0046(u q sametime i)\n",
    "#0.8531    0.335   \n",
    "#0.847              0.81828      qitime_ratio稳定上0.005\n",
    "#0.8502   0.33774   0.822 0.0282 today中关于q的特征\n",
    "#0.8537    0.33477   0.811922 today中关于u的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id=pd.read_csv('invite_info_evaluate_2_0926.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "test_id['target']=sub1\n",
    "test_id.to_csv('result_8.txt',index=False, sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028200000000000003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8502-0.822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
