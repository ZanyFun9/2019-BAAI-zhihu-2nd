{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from deepctr.layers.utils import Hash,concat_fun\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from deepctr.layers import *\n",
    "from tensorflow.python.keras.callbacks import *\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.models import *\n",
    "from tensorflow.python.keras.initializers import *\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sum(input_tensor,axis=None,keep_dims=False,name=None,reduction_indices=None):\n",
    "    if tf.__version__ < '2.0.0':\n",
    "        return tf.reduce_sum(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keep_dims=keep_dims,\n",
    "                   name=name,\n",
    "                   reduction_indices=reduction_indices)\n",
    "    else:\n",
    "        return  tf.reduce_sum(input_tensor,\n",
    "                   axis=axis,\n",
    "                   keepdims=keep_dims,\n",
    "                   name=name)\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, l2_reg=0.0, mode=0, **kwargs):\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        # self.l2_reg = tf.contrib.layers.l2_regularizer(float(l2_reg_linear))\n",
    "        self.mode = mode\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.bias = self.add_weight(name='linear_bias',\n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Zeros(),\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=1, activation=None, use_bias=False,\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg))\n",
    "\n",
    "        super(Linear, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs , **kwargs):\n",
    "\n",
    "        if self.mode == 0:\n",
    "            sparse_input = inputs\n",
    "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)\n",
    "        elif self.mode == 1:\n",
    "            dense_input = inputs\n",
    "            linear_logit = self.dense(dense_input)\n",
    "\n",
    "        else:\n",
    "            sparse_input, dense_input = inputs\n",
    "\n",
    "            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + self.dense(dense_input)\n",
    "\n",
    "        linear_bias_logit = linear_logit + self.bias\n",
    "\n",
    "        return linear_bias_logit\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'mode': self.mode, 'l2_reg': self.l2_reg}\n",
    "        base_config = super(Linear, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'dimension', 'use_hash', 'dtype','embedding_name','embedding'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension, use_hash=False, dtype=\"int32\", embedding_name=None,embedding=True):\n",
    "        if embedding and embedding_name is None:\n",
    "            embedding_name = name\n",
    "        return super(SparseFeat, cls).__new__(cls, name, dimension, use_hash, dtype, embedding_name,embedding)\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n",
    "\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n",
    "class VarLenSparseFeat(namedtuple('VarLenFeat', ['name', 'dimension', 'maxlen', 'combiner', 'use_hash', 'dtype','embedding_name','embedding'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension, maxlen, combiner=\"mean\", use_hash=False, dtype=\"float32\", embedding_name=None,embedding=True):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, name, dimension, maxlen, combiner, use_hash, dtype, embedding_name,embedding)\n",
    "def get_fixlen_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns, include_varlen=False,include_fixlen=True)\n",
    "    return list(features.keys())\n",
    "def get_varlen_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns, include_varlen=True,include_fixlen=False)\n",
    "    return list(features.keys())\n",
    "def get_inputs_list(inputs):\n",
    "    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n",
    "def build_input_features(feature_columns, include_varlen=True, mask_zero=True, prefix='',include_fixlen=True):\n",
    "    input_features = OrderedDict()\n",
    "    if include_fixlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,SparseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(1,), name=prefix+fc.name, dtype=fc.dtype)\n",
    "            elif isinstance(fc,DenseFeat):\n",
    "                input_features[fc.name] = Input(\n",
    "                    shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n",
    "    if include_varlen:\n",
    "        for fc in feature_columns:\n",
    "            if isinstance(fc,VarLenSparseFeat):\n",
    "                input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + 'seq_' + fc.name,\n",
    "                                                      dtype=fc.dtype)\n",
    "        if not mask_zero:\n",
    "            for fc in feature_columns:\n",
    "                input_features[fc.name+\"_seq_length\"] = Input(shape=(\n",
    "                    1,), name=prefix + 'seq_length_' + fc.name)\n",
    "                input_features[fc.name+\"_seq_max_length\"] = fc.maxlen\n",
    "\n",
    "\n",
    "    return input_features\n",
    "def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed, l2_reg,prefix='sparse_', seq_mask_zero=True):\n",
    "    if embedding_size == 'auto':\n",
    "        print(\"Notice:Do not use auto embedding in models other than DCN\")\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(l2_reg),\n",
    "                                                 name=prefix + '_emb_' + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "    else:\n",
    "\n",
    "        sparse_embedding = {feat.embedding_name: Embedding(feat.dimension, embedding_size,\n",
    "                                                 embeddings_initializer=RandomNormal(\n",
    "                                                     mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                 embeddings_regularizer=l2(\n",
    "                                                     l2_reg),\n",
    "                                                 name=prefix + '_emb_'  + feat.name) for feat in\n",
    "                            sparse_feature_columns}\n",
    "\n",
    "    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n",
    "        for feat in varlen_sparse_feature_columns:\n",
    "            # if feat.name not in sparse_embedding:\n",
    "            if embedding_size == \"auto\":\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, 6 * int(pow(feat.dimension, 0.25)),\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "            else:\n",
    "                sparse_embedding[feat.embedding_name] = Embedding(feat.dimension, embedding_size,\n",
    "                                                        embeddings_initializer=RandomNormal(\n",
    "                                                            mean=0.0, stddev=init_std, seed=seed),\n",
    "                                                        embeddings_regularizer=l2(\n",
    "                                                            l2_reg),\n",
    "                                                        name=prefix + '_seq_emb_' + feat.name,\n",
    "                                                        mask_zero=seq_mask_zero)\n",
    "\n",
    "\n",
    "    return sparse_embedding\n",
    "def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fg in sparse_feature_columns:\n",
    "        feat_name = fg.name\n",
    "        if len(return_feat_list) == 0  or feat_name in return_feat_list:\n",
    "            if fg.use_hash:\n",
    "                lookup_idx = Hash(fg.dimension,mask_zero=(feat_name in mask_feat_list))(input_dict[feat_name])\n",
    "            else:\n",
    "                lookup_idx = input_dict[feat_name]\n",
    "\n",
    "            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list\n",
    "def create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=\"\",seq_mask_zero=True):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat) and x.embedding, feature_columns)) if feature_columns else []\n",
    "    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, embedding_size, init_std, seed,\n",
    "                                                 l2_reg, prefix=prefix + 'sparse',seq_mask_zero=seq_mask_zero)\n",
    "    return sparse_emb_dict\n",
    "def get_linear_logit(features, feature_columns, units=1, l2_reg=0, init_std=0.0001, seed=1024, prefix='linear'):\n",
    "\n",
    "    linear_emb_list = [input_from_feature_columns(features,feature_columns,1,l2_reg,init_std,seed,prefix=prefix+str(i))[0] for i in range(units)]\n",
    "    _, dense_input_list = input_from_feature_columns(features,feature_columns,1,l2_reg,init_std,seed,prefix=prefix)\n",
    "\n",
    "    linear_logit_list = []\n",
    "    for i in range(units):\n",
    "\n",
    "        if len(linear_emb_list[0])>0 and len(dense_input_list) >0:\n",
    "            sparse_input = concat_fun(linear_emb_list[i])\n",
    "            dense_input = concat_fun(dense_input_list)\n",
    "            linear_logit = Linear(l2_reg,mode=2)([sparse_input,dense_input])\n",
    "        elif len(linear_emb_list[0])>0:\n",
    "            sparse_input = concat_fun(linear_emb_list[i])\n",
    "            linear_logit = Linear(l2_reg,mode=0)(sparse_input)\n",
    "        elif len(dense_input_list) >0:\n",
    "            dense_input = concat_fun(dense_input_list)\n",
    "            linear_logit = Linear(l2_reg,mode=1)(dense_input)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        linear_logit_list.append(linear_logit)\n",
    "\n",
    "    return concat_fun(linear_logit_list)\n",
    "def embedding_lookup(sparse_embedding_dict,sparse_input_dict,sparse_feature_columns,return_feat_list=(), mask_feat_list=()):\n",
    "    embedding_vec_list = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if len(return_feat_list) == 0  or feature_name in return_feat_list and fc.embedding:\n",
    "            if fc.use_hash:\n",
    "                lookup_idx = Hash(fc.dimension,mask_zero=(feature_name in mask_feat_list))(sparse_input_dict[feature_name])\n",
    "            else:\n",
    "                lookup_idx = sparse_input_dict[feature_name]\n",
    "\n",
    "            embedding_vec_list.append(sparse_embedding_dict[embedding_name](lookup_idx))\n",
    "\n",
    "    return embedding_vec_list\n",
    "def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            lookup_idx = Hash(fc.dimension, mask_zero=True)(sequence_input_dict[feature_name])\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n",
    "\n",
    "    return varlen_embedding_vec_dict\n",
    "def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns):\n",
    "    pooling_vec_list = []\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        combiner = fc.combiner\n",
    "        feature_length_name = feature_name + '_seq_length'\n",
    "        if feature_length_name in features:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n",
    "            [embedding_dict[feature_name], features[feature_length_name]])\n",
    "        else:\n",
    "            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n",
    "            embedding_dict[feature_name])\n",
    "        pooling_vec_list.append(vec)\n",
    "    return pooling_vec_list\n",
    "def get_dense_input(features,feature_columns):\n",
    "    dense_feature_columns = list(filter(lambda x:isinstance(x,DenseFeat),feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        dense_input_list.append(features[fc.name])\n",
    "    return dense_input_list\n",
    "def input_from_feature_columns(features,feature_columns, embedding_size, l2_reg, init_std, seed,prefix='',seq_mask_zero=True,support_dense=True):\n",
    "\n",
    "\n",
    "    sparse_feature_columns = list(filter(lambda x:isinstance(x,SparseFeat),feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    embedding_dict = create_embedding_matrix(feature_columns,l2_reg,init_std,seed,embedding_size, prefix=prefix,seq_mask_zero=seq_mask_zero)\n",
    "    sparse_embedding_list = embedding_lookup(\n",
    "        embedding_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features,feature_columns)\n",
    "    if not support_dense and len(dense_value_list) >0:\n",
    "        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_dict,features,varlen_sparse_feature_columns)\n",
    "    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, varlen_sparse_feature_columns)\n",
    "    sparse_embedding_list += sequence_embed_list\n",
    "\n",
    "    return sparse_embedding_list, dense_value_list\n",
    "def combined_dnn_input(sparse_embedding_list,dense_value_list):\n",
    "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
    "        sparse_dnn_input = Flatten()(concat_fun(sparse_embedding_list))\n",
    "        dense_dnn_input = Flatten()(concat_fun(dense_value_list))\n",
    "        return concat_fun([sparse_dnn_input,dense_dnn_input])\n",
    "    elif len(sparse_embedding_list) > 0:\n",
    "        return Flatten()(concat_fun(sparse_embedding_list))\n",
    "    elif len(dense_value_list) > 0:\n",
    "        return Flatten()(concat_fun(dense_value_list))\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the JSON object must be str, not 'bytes'\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_1.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "feature_answer_base=pd.read_csv('feature_answer_base.csv')\n",
    "# member_info=pd.read_csv('member_info.txt',sep='\\s+',names=['uid','sex','key_word','num_level','hot_level','regis_type','regis_platform',\n",
    "#                                                           'look_freq','a','b','c','d','e','A','B','C','D','E','salt','l_topic','topic_n'])\n",
    "# ques_info=pd.read_csv('question_info.txt',sep='\\s+',names=['qid','qtime','qtitle','qtitlec','qinfo','qinfoc','qtopic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[['qid','uid','target']]\n",
    "test=test[['qid','uid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['picture_max','video_max','video_min','collect_max','collect_min',\n",
    "          'report_mean','report_max','report_sum','report_min','report_std',\n",
    "          'nohelp_mean','nohelp_max','nohelp_min','nohelp_std','oppose_max',\n",
    "          'oppose_sum','oppose_std']\n",
    "feature_answer_base.drop(drop_col,axis=1,inplace=True)\n",
    "train=pd.merge(train,feature_answer_base,how='left',on='uid')\n",
    "test=pd.merge(test,feature_answer_base,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic_count=pd.read_csv('feature_topic_count.csv')\n",
    "train=pd.concat([train,feature_topic_count.iloc[0:train.shape[0],:]],axis=1)\n",
    "test=pd.concat([test,feature_topic_count.iloc[train.shape[0]:,:].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic_tfidf=pd.read_csv('feature_topic_tfidf.csv')\n",
    "feature_topic_w2vdis=pd.read_csv('feature_topic_w2vdis.csv')\n",
    "feature_qid_freq=pd.read_csv('feature_qid_freq.csv')\n",
    "feature_uid_freq=pd.read_csv('feature_uid_freq.csv')\n",
    "feature_uid_ctr=pd.read_csv('feature_uid_ctr.csv')\n",
    "feature_uid_ctr=pd.read_csv('feature_uid_ctr.csv')\n",
    "feature_member_info=pd.read_csv('feature_member_info.csv')\n",
    "\n",
    "train=pd.concat([train,\n",
    "                 feature_topic_tfidf.iloc[0:train.shape[0],:],\n",
    "                 feature_topic_w2vdis.iloc[0:train.shape[0],:],\n",
    "                 feature_qid_freq.iloc[0:train.shape[0],:],\n",
    "                 feature_uid_freq.iloc[0:train.shape[0],:],\n",
    "                 feature_uid_ctr.iloc[0:train.shape[0],:],\n",
    "                 feature_member_info.iloc[0:train.shape[0],:],],axis=1)\n",
    "test=pd.concat([test,\n",
    "                feature_topic_tfidf.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_topic_w2vdis.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_qid_freq.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_uid_freq.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_uid_ctr.iloc[train.shape[0]:,:].reset_index(drop=True),\n",
    "                feature_member_info.iloc[train.shape[0]:,:].reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_answer_count=pd.read_csv('feature_answer_count.csv')\n",
    "train=pd.merge(train,feature_answer_count,how='left',on='uid')\n",
    "test=pd.merge(test,feature_answer_count,how='left',on='uid')\n",
    "\n",
    "feature_topic_answer=pd.read_csv('feature_topic_answer.csv')\n",
    "train=pd.merge(train,feature_topic_answer,how='left',on='qid')\n",
    "test=pd.merge(test,feature_topic_answer,how='left',on='qid')\n",
    "\n",
    "feature_topicn_tongji=pd.read_csv('feature_topicn_tongji.csv')\n",
    "train=pd.merge(train,feature_topicn_tongji,how='left',on='uid')\n",
    "test=pd.merge(test,feature_topicn_tongji,how='left',on='uid')\n",
    "\n",
    "feature_answer_percount=pd.read_csv('feature_answer_percount.csv')\n",
    "train=pd.merge(train,feature_answer_percount,how='left',on='uid')\n",
    "test=pd.merge(test,feature_answer_percount,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic_meta=pd.read_csv('feature_topic_meta.csv').drop_duplicates(['qid','uid'])\n",
    "train=pd.merge(train,feature_topic_meta,how='left',on=['qid','uid'])\n",
    "test=pd.merge(test,feature_topic_meta,how='left',on=['qid','uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11421517"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alldata=pd.concat([train.drop(['target'],axis=1),test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=['sex','look_freq','a','b','c','d','e','A','B','C','D','E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = [i for i in alldata.columns if i not in cat_col + ['uid','qid']]\n",
    "sparse_features = [i for i in alldata.columns if i not in dense_features + ['uid','qid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'look_freq', 'a', 'b', 'c', 'd', 'e', 'A', 'B', 'C', 'D', 'E']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = StandardScaler()\n",
    "alldata[dense_features] = mm.fit_transform(alldata[dense_features].replace([np.inf, -np.inf], 0).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 68)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, alldata[feat].nunique()) for feat in sparse_features] + [\n",
    "    DenseFeat(feat, 1, ) for feat in dense_features]\n",
    "linear_feature_columns = fixlen_feature_columns #+ varlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns #+ varlen_feature_columns\n",
    "fixlen_feature_names = get_fixlen_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "varlen_feature_names = get_varlen_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "len(sparse_features), len(dense_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xDeepFM(linear_feature_columns, dnn_feature_columns, embedding_size=8, dnn_hidden_units=(512, 256),\n",
    "            cin_layer_size=(256, 256,), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.00001,\n",
    "            l2_reg_embedding=0.00001, l2_reg_dnn=0, l2_reg_cin=0, init_std=0.0001, seed=2019, dnn_dropout=0,\n",
    "            dnn_activation='relu', dnn_use_bn=False, task='binary'):\n",
    "\n",
    "    features = build_input_features(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    datas_list = list(features.values())\n",
    "\n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                        embedding_size,\n",
    "                                                                        l2_reg_embedding, init_std,\n",
    "                                                                        seed)\n",
    "\n",
    "    #     linear_logit\n",
    "    feature_columns = linear_feature_columns\n",
    "    prefix = 'linear'\n",
    "    units = 6\n",
    "    l2_reg = l2_reg_linear\n",
    "    linear_emb_list = [\n",
    "        input_from_feature_columns(features, feature_columns, 1, l2_reg, init_std, seed, prefix=prefix + str(i))[0] for i\n",
    "        in range(units)]\n",
    "    _, dense_data_list = input_from_feature_columns(features, feature_columns, 1, l2_reg, init_std, seed, prefix=prefix)\n",
    "\n",
    "    if len(linear_emb_list[0]) > 1:\n",
    "        linear_term = concat_fun([tf.keras.layers.add(linear_emb) for linear_emb in linear_emb_list])\n",
    "    elif len(linear_emb_list[0]) == 1:\n",
    "        linear_term = concat_fun([linear_emb[0] for linear_emb in linear_emb_list])\n",
    "    else:\n",
    "        linear_term = None\n",
    "\n",
    "    if len(dense_data_list) > 0:\n",
    "        dense_data__ = dense_data_list[0] if len(\n",
    "            dense_data_list) == 1 else tf.keras.layers.Concatenate()(dense_data_list)\n",
    "        linear_dense_logit = tf.keras.layers.Dense(\n",
    "            units, activation='softplus', use_bias=True, kernel_regularizer=l2(l2_reg))(dense_data__)\n",
    "\n",
    "        if linear_term is not None:\n",
    "            linear_term = tf.keras.layers.add([linear_dense_logit, linear_term])\n",
    "        else:\n",
    "            linear_term = linear_dense_logit\n",
    "\n",
    "    linear_logit = tf.keras.layers.Flatten()(linear_term)\n",
    "\n",
    "    fm_data = concat_fun(sparse_embedding_list, axis=1)\n",
    "\n",
    "    if len(cin_layer_size) > 0:\n",
    "        exFM_out = CIN(cin_layer_size, cin_activation,\n",
    "                       cin_split_half, l2_reg_cin, seed)(fm_data)\n",
    "        exFM_logit = tf.keras.layers.Dense(6, activation='softplus', )(exFM_out)\n",
    "        exFM_logit_reg = tf.keras.layers.Dense(1, activation='relu')(exFM_out)\n",
    "\n",
    "    dnn_data_1 = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "\n",
    "    deep_out_1 = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
    "                     dnn_use_bn, seed)(dnn_data_1)\n",
    "\n",
    "    deep_logit_1 = tf.keras.layers.Dense(\n",
    "        1, use_bias=False, activation='sigmoid')(deep_out_1)\n",
    "\n",
    "    x = tf.keras.layers.average([exFM_logit, linear_logit, deep_logit_1])\n",
    "    x = tf.keras.layers.concatenate([x, exFM_logit_reg])\n",
    "   # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.keras.layers.PReLU()(x)\n",
    "  #  x = tf.keras.layers.BatchNormalization()(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inputs=datas_list, outputs=output)#datas\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(JB, index):\n",
    "    JB = JB.iloc[index]\n",
    "    fixlen_data = [JB[name].values for name in fixlen_feature_names]\n",
    "\n",
    "    # v2 = [app1_list[index]]\n",
    "    # v3 = [app1_list[index]]\n",
    "    return fixlen_data #+ v2 + v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10166100, 82) (1255417, 82)\n"
     ]
    }
   ],
   "source": [
    "X_train = alldata.iloc[0:train.shape[0],:].reset_index(drop=True)\n",
    "y = target.astype(int)\n",
    "X_test = alldata.iloc[train.shape[0]:,:].reset_index(drop=True)\n",
    "print(X_train.shape, X_test.shape)\n",
    "cv_pred = []\n",
    "test_pred = []\n",
    "cv_score = []\n",
    "cv_model = []\n",
    "sub1 = np.zeros((test.shape[0],1 ))\n",
    "oof_pref1 = np.zeros((train.shape[0], 1))\n",
    "count=0\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 799999 samples, validate on 200001 samples\n",
      "Epoch 1/10\n",
      "799999/799999 [==============================] - 144s 180us/step - loss: 0.4020 - acc: 0.8313 - val_loss: 0.3908 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39076, saving model to xdeepfm_best_model0.h5\n",
      "Epoch 2/10\n",
      "799999/799999 [==============================] - 124s 155us/step - loss: 0.3866 - acc: 0.8353 - val_loss: 0.3818 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39076 to 0.38180, saving model to xdeepfm_best_model0.h5\n",
      "Epoch 3/10\n",
      " 99328/799999 [==>...........................] - ETA: 1:46 - loss: 0.3785 - acc: 0.8371"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e959f613bcb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msub1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2859\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    659\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(skf.split(X_train.iloc[0:1000000], y.iloc[0:1000000])):\n",
    "    #K.clear_session()\n",
    "    filepath = \"xdeepfm_best_model%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=1,verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    print(index)\n",
    "    model = xDeepFM(linear_feature_columns, dnn_feature_columns, embedding_size=8,\n",
    "                    task='binary')  # xDeepFM DeepFM AFM NFM\n",
    "    # model.compile(RAdam(lr=0.01), 'categorical_crossentropy',\n",
    "    #               metrics=['accuracy', ], )\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'], )\n",
    "    train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[\n",
    "        test_index]\n",
    "#     train_x=train_x.iloc[0:1000000]\n",
    "#     train_y=train_y.iloc[0:1000000]\n",
    "    train_x = make_data(X_train, train_index) \n",
    "    train_y = train_y\n",
    "    test_x = make_data(X_train, test_index)\n",
    "    test_y = test_y\n",
    "    test_data = make_data(alldata, range(len(train), len(alldata)))\n",
    "    history = model.fit(train_x, train_y, batch_size=512, epochs=10, verbose=1, validation_data=(test_x, test_y),callbacks=callbacks,)\n",
    "    model.load_weights(filepath)\n",
    "    sub1 += model.predict(test_data, batch_size=2048)\n",
    "    oof_pref1[test_index] = model.predict(test_x, batch_size=2048)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0:1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 2., 1., 1.]),\n",
       " array([1., 0., 2., ..., 1., 1., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 1., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([ 8.,  0.,  0., ...,  0., 64., 26.]),\n",
       " array([3., 0., 0., ..., 0., 5., 6.]),\n",
       " array([ 7., 15.,  7., ..., 37.,  3.,  7.]),\n",
       " array([87.,  5., 84., ...,  0.,  0., 83.]),\n",
       " array([0., 0., 0., ..., 0., 1., 0.]),\n",
       " array([ 2.12825867, -0.40645519, -0.40645519, ..., -0.40645519,\n",
       "        -0.01649921, -0.40645519]),\n",
       " array([ 0.15598875, -0.21933451, -0.21933451, ..., -0.21933451,\n",
       "        -0.03167288, -0.21933451]),\n",
       " array([-0.14243624, -0.14243624, -0.14243624, ..., -0.14243624,\n",
       "        -0.14243624, -0.14243624]),\n",
       " array([ 2.60485737, -0.52011453, -0.52011453, ..., -0.52011453,\n",
       "         0.98107341, -0.52011453]),\n",
       " array([-0.10439391, -0.10439391, -0.10439391, ..., -0.10439391,\n",
       "         1.73123637, -0.10439391]),\n",
       " array([-0.06725602, -0.06725602, -0.06725602, ..., -0.06725602,\n",
       "         0.94676402, -0.06725602]),\n",
       " array([-0.15243616, -0.15243616, -0.15243616, ..., -0.15243616,\n",
       "         4.78605652, -0.15243616]),\n",
       " array([ 0.01265274, -0.44165061, -0.44165061, ..., -0.42769887,\n",
       "         1.19070232,  0.17443637]),\n",
       " array([-0.20019588, -0.34706871, -0.34706871, ..., -0.34385838,\n",
       "         2.67706694,  1.46516544]),\n",
       " array([-0.18542975, -0.2404118 , -0.2404118 , ..., -0.23998967,\n",
       "         0.40164345,  1.08306205]),\n",
       " array([ 0.19287274, -0.23708518, -0.23708518, ..., -0.21021281,\n",
       "        -0.19677663, -0.21021281]),\n",
       " array([-0.20904429, -0.35890098, -0.35890098, ..., -0.35890098,\n",
       "         2.73937333,  0.56302739]),\n",
       " array([-0.04777173, -0.06058415, -0.06058415, ..., -0.06058415,\n",
       "         0.10183794, -0.02983434]),\n",
       " array([-0.07854287, -0.08222215, -0.08222215, ..., -0.08222215,\n",
       "         0.16061064,  0.00362782]),\n",
       " array([-0.08545053, -0.0898953 , -0.0898953 , ..., -0.0898953 ,\n",
       "         0.09322959,  0.09945228]),\n",
       " array([-0.0123131, -0.0123131, -0.0123131, ..., -0.0123131, -0.0123131,\n",
       "        -0.0123131]),\n",
       " array([-0.06955558, -0.07593359, -0.07593359, ..., -0.07593359,\n",
       "         0.20151345, -0.02173984]),\n",
       " array([-0.0098637 , -0.07954688, -0.07954688, ..., -0.07954688,\n",
       "        -0.03666493,  0.04215332]),\n",
       " array([-0.07299786, -0.10716872, -0.10716872, ..., -0.10716872,\n",
       "        -0.03882701,  0.13202728]),\n",
       " array([-0.10323766, -0.12437655, -0.12437655, ..., -0.12437655,\n",
       "        -0.08209877,  0.53092911]),\n",
       " array([-0.01058543, -0.01058543, -0.01058543, ..., -0.01058543,\n",
       "        -0.01058543, -0.01058543]),\n",
       " array([-0.03273023, -0.11980033, -0.11980033, ..., -0.11980033,\n",
       "        -0.02320472,  0.08985588]),\n",
       " array([ 0.02352158, -0.1085883 , -0.1085883 , ..., -0.1085883 ,\n",
       "         0.10046469, -0.00651548]),\n",
       " array([-0.09724281, -0.1418966 , -0.1418966 , ..., -0.1418966 ,\n",
       "         0.11486268,  0.02555511]),\n",
       " array([-0.12376613, -0.17210071, -0.17210071, ..., -0.17210071,\n",
       "         0.07647714,  0.49077355]),\n",
       " array([-0.01891608, -0.01891608, -0.01891608, ..., -0.01891608,\n",
       "        -0.01891608, -0.01891608]),\n",
       " array([-0.05858027, -0.13260479, -0.13260479, ..., -0.13260479,\n",
       "         0.13989843,  0.00467693]),\n",
       " array([-0.0207043 , -0.0207043 , -0.0207043 , ..., -0.0207043 ,\n",
       "        -0.00644266, -0.01796867]),\n",
       " array([-0.03878779, -0.03878779, -0.03878779, ..., -0.03878779,\n",
       "        -0.01567187, -0.01457111]),\n",
       " array([-0.03003542, -0.03003542, -0.03003542, ..., -0.03003542,\n",
       "        -0.00217083, -0.02474516]),\n",
       " array([-0.02461963, -0.03546589, -0.03546589, ..., -0.03546589,\n",
       "         0.02460575, -0.02324475]),\n",
       " array([-0.04155869, -0.04710351, -0.04710351, ..., -0.04710351,\n",
       "         0.05270313, -0.02492425]),\n",
       " array([-0.05709503, -0.06180293, -0.06180293, ..., -0.06180293,\n",
       "         0.02293932,  0.03235512]),\n",
       " array([-0.00649616, -0.00649616, -0.00649616, ..., -0.00649616,\n",
       "        -0.00649616, -0.00649616]),\n",
       " array([-0.03222982, -0.04304935, -0.04304935, ..., -0.04304935,\n",
       "         0.06497935, -0.02621489]),\n",
       " array([-0.0479133, -0.0479133, -0.0479133, ..., -0.0479133, -0.0479133,\n",
       "        -0.0479133]),\n",
       " array([-0.0098637 , -0.07954688, -0.07954688, ..., -0.07954688,\n",
       "        -0.03666493,  0.04215332]),\n",
       " array([-0.01058543, -0.01058543, -0.01058543, ..., -0.01058543,\n",
       "        -0.01058543, -0.01058543]),\n",
       " array([ 1.09311424,  0.54485489, -0.21427345, ..., -0.21427345,\n",
       "        -0.76253281,  1.30398323]),\n",
       " array([ 1.08674659, -1.36137048,  1.90278561, ..., -0.54533146,\n",
       "         0.27070757, -1.36137048]),\n",
       " array([-1.18594924, -0.68279096,  0.14278408, ...,  0.80606581,\n",
       "        -0.18702375, -0.04284631]),\n",
       " array([-0.98770037, -0.82224198,  0.76791445, ...,  1.15049481,\n",
       "        -0.20577309, -0.5134079 ]),\n",
       " array([-1.34997828, -0.57953716,  0.21401411, ...,  0.80194786,\n",
       "        -0.20649043,  0.56750123]),\n",
       " array([-1.1175964 ,  1.05382625,  1.22694737, ...,  0.18580179,\n",
       "         0.3693542 , -0.08203456]),\n",
       " array([-1.12677144, -0.98380371, -0.58140718, ...,  0.42655061,\n",
       "         0.2982245 , -0.73083578]),\n",
       " array([ 1.3372983 ,  0.45796015, -0.90500409, ...,  1.59512394,\n",
       "         0.92742693, -1.60556292]),\n",
       " array([ 1.42698499, -0.03042038, -1.51313543, ...,  1.61317727,\n",
       "         1.32210488, -0.80765574]),\n",
       " array([ 0.46729686, -0.40847221, -1.0870803 , ...,  0.40755368,\n",
       "         0.25035557,  1.69649325]),\n",
       " array([ 1.34315447,  0.38384018, -1.67763855, ...,  1.59613362,\n",
       "         0.9169685 , -0.69766012]),\n",
       " array([ 1.27502929,  0.71329327, -1.72360219, ...,  1.49133163,\n",
       "         0.61196078, -0.57735538]),\n",
       " array([ 1.64443461, -0.31008981, -1.74194922, ...,  2.0270982 ,\n",
       "         0.88434645,  0.61956337]),\n",
       " array([-0.21397308, -0.20888338, -0.19361427, ..., -0.18089001,\n",
       "        -0.22415248, -0.21142823]),\n",
       " array([-0.30038229, -0.61741298, -0.39096248, ..., -0.43625258,\n",
       "        -0.16451199,  1.78296225]),\n",
       " array([-0.34732513, -0.88909477,  1.66496213, ...,  0.60077175,\n",
       "        -0.88909477, -0.59112147]),\n",
       " array([-0.06193855, -0.93043811,  0.76708375, ..., -0.15668396,\n",
       "         0.10386591,  1.28818348]),\n",
       " array([-0.33622168, -0.49387782, -0.49387782, ..., -0.45446378,\n",
       "         0.01850462,  2.30451859]),\n",
       " array([ 3.37274598e-01, -5.93102501e-01, -4.38557124e-01, ...,\n",
       "        -3.69038997e-04, -5.36330434e-01, -6.44143680e-01]),\n",
       " array([ 0.7351196 ,  0.46462213,  0.07512451, ..., -0.8994925 ,\n",
       "         0.1370557 ,  1.19188555]),\n",
       " array([ 0.79038216,  0.54378169,  0.18869411, ..., -1.18229607,\n",
       "         0.24515401,  1.20679527]),\n",
       " array([ 0.32995381,  0.41879017,  0.30663049, ..., -0.80220249,\n",
       "         0.27295876,  0.88218173]),\n",
       " array([-0.785766  , -0.11644778,  0.35235143, ...,  0.43930481,\n",
       "         0.35645528, -0.238847  ]),\n",
       " array([-0.36094324, -0.36705557, -0.36705557, ...,  1.07392459,\n",
       "        -0.33562479, -0.28859729]),\n",
       " array([-0.36094324, -0.36705557, -0.36705557, ...,  1.07392459,\n",
       "        -0.33562479, -0.28859729]),\n",
       " array([-0.32761608, -1.52079019,  1.528645  , ...,  0.50216681,\n",
       "         0.07270198, -1.67498909]),\n",
       " array([-0.0052492 , -0.89131951,  0.92233643, ...,  0.85628589,\n",
       "        -0.38271393, -0.89131951]),\n",
       " array([-0.44111593, -1.42582722,  1.04769609, ...,  0.43833685,\n",
       "         0.18639623, -1.59206013]),\n",
       " array([-0.27856452, -1.53709382,  1.47205726, ...,  0.52828495,\n",
       "         0.00398313, -1.67456477]),\n",
       " array([-0.29355357, -1.51436029,  1.64436987, ...,  0.50364941,\n",
       "         0.03819875, -1.66818209])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
